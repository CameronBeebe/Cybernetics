{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybernetic Game Theory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS ACTIVELY W.I.P. August-November 2020.  Began transforming sketch into functions.  \n",
    "# Instead of deleting sketch work, I am just commenting out to preserve thought process.\n",
    "# Next step is to separate out functions and create new notebook/script to continue development.\n",
    "\n",
    "# This notebook aims to illustrate a toy model of a cybernetic regulator along the lines of W.R. Ashby's work.\n",
    "# It is instructive to see the game-theoretic foundations of other popular regulators, like Artificial Neural Networks.\n",
    "# The regulator can \"learn\" a probability distribution of disturbances, using reinforcement learning.\n",
    "# The result is effective control, channeling the flow of information from the environment into desired outcomes (states)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a game matrix for two players: Environment and Regulator\n",
    "# Choose a goal for Regulator\n",
    "# Environment goes first (row_i)\n",
    "# Regulator goes second (column_j)\n",
    "# Outcome is matrix element m_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game_matrix = np.random.randint(10, size=(7,5))\n",
    "#game_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows are plays (a.k.a. \"disturbances\") for the environment.  \n",
    "# Create vector to use for pandas index and later to link up with probabilities.\n",
    "def create_game(size):\n",
    "    game_matrix = np.random.randint(10, size=size)\n",
    "    rows = [i+1 for i in range(len(game_matrix))]\n",
    "    print(rows)\n",
    "    return pd.DataFrame(data = game_matrix, columns=[i+1 for i in range(game_matrix.shape[1])], index=rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = create_game((7,12))\n",
    "#game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment chooses play (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution for environmental \"plays\" or \"disturbances\".\n",
    "#dist = np.random.dirichlet(alpha=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have probabilities summing to 1.\n",
    "#print(dist)\n",
    "#sum(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a play.  Distribution of environment must remain constant!\n",
    "# There must be something to learn, and if dist is changing, the world is random.\n",
    "def environment_play(game,dist):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    return np.random.choice(game.index, size=1, p=dist).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist = np.random.dirichlet(alpha=game.index)\n",
    "#env_play = environment_play(game,dist)\n",
    "#env_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regulator chooses action (column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a Polya urn instead to define probabilities of actions for the regulator.\n",
    "#urn = np.random.randint(10, size=len(game.columns))\n",
    "#urn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities of drawing from urn\n",
    "#probs = np.array([(i/sum(urn)) for i in urn])\n",
    "#probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random draws from plays in the urn with probabilities according to the composition of the urn.\n",
    "# We actually just care to draw from the plays, and not from the urn itself, although the urn is what will be updated/reinforced.\n",
    "def regulator_action(game,probs):\n",
    "    return np.random.choice(game.columns, size=1, p=probs).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_action = regulator_action(game,probs)\n",
    "#reg_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update/Reinforce the action of the regulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .item to get the value from the game table out of the locations (rows/columns) encoded in arrays.\n",
    "#out = game.loc[env_play,reg_action]\n",
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convince yourself that this row/column outcome corresponds to the game.\n",
    "#game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We need to set a goal for the regulator to achieve.\n",
    "#goal = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regulator as dictionary of plays and associated probabilities.\n",
    "#regulator = dict(zip(game.columns,urn))\n",
    "#regulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outcome with goal, and reinforce (increase probability) action which regulator took in response to environment.\n",
    "# It makes sense to reinforce individual successes more than we weaken, since we expect to fail a lot in the beginning.\n",
    "# Try adding len(regulator), although this may be impractical for some examples.\n",
    "#if out == goal:\n",
    "#    print(\"success: reinforced the regulator's action\", reg_action, \"from\", regulator[reg_action], \"to\", regulator[reg_action] + len(regulator))\n",
    "#    regulator[reg_action] += len(regulator)\n",
    "#    print('now we need to recalculate the probabilities according to the reinforced urn')\n",
    "#else: print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm updated urn.\n",
    "#regulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate probabilities and confirm that correct play increases in probability and all others go down.\n",
    "def prob_calc(regulator_dict):\n",
    "    sum_reg = np.array(sum([regulator_dict[i] for i in regulator_dict]))\n",
    "    return np.array([regulator_dict[i]/sum_reg for i in regulator_dict])\n",
    "    #sum(updated_probs),updated_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_calc(regulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original probs\n",
    "#probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want the regulator to update when it fails.  How might this look?\n",
    "# Remember that the regulator's action distribution was random (the composition of the urn).  \n",
    "# Consider that the regulator was perhaps overconfident, how can we make the \"urn\" less \"confident\"?\n",
    "# Lets \"sqeeze\" the distribution in the urn, by making it less confident for any particular action.\n",
    "\n",
    "# We could make drastic changes to our distribution, but we should keep in mind that we may want to use \n",
    "# the same fail-update procedure over and over again in an automated learning process.  \n",
    "# If our \"squeeze\" is too drastic (e.g. making the urn parts equal / probabilities uniform) then even if\n",
    "# our regulator is \"learned\" but makes a mistake, it will \"forget\" the learned distribution.\n",
    "\n",
    "# We can still use the mean of the urn composition in a function that slightly squeezes, \n",
    "# incrementing those actions which are below the mean, and decrementing those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: IS SQUEEZING ALWAYS WORKING PROPERLY?  NEEDS EXPERIMENTATION\n",
    "# Notice how multiple (failing) applications of this function will \"converge\" around the mean.\n",
    "# More precisely, it will slightly oscillate around the convergence point.\n",
    "def squeeze(regulator_dict, urn_list):\n",
    "    '''\n",
    "    This update function takes two arguments.\n",
    "    \n",
    "    regulator_dict: a regulator defined as a dictionary of key labels (plays or columns) \n",
    "    and integer values (from a distribution or urn).\n",
    "    \n",
    "    urn_list: a list of integers interpreted as the composition of a Polya urn.\n",
    "    \n",
    "    The function calculates the mean of the urn composition, and compares each value\n",
    "    in the regulator_dict with the mean.  \n",
    "    \n",
    "    The resulting regulator_dict is updated by incrementing values smaller than the\n",
    "    mean, and decrementing values greater than the mean.\n",
    "    \n",
    "    '''\n",
    "    mean = np.mean(urn_list)\n",
    "    for i in regulator_dict:\n",
    "        if regulator_dict[i] >= mean:\n",
    "            regulator_dict[i] -= 1\n",
    "            #print('squeeze down:', regulator_dict[i])\n",
    "        else:\n",
    "            regulator_dict[i] += 1\n",
    "            #print('squeeze up:', regulator_dict[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our new function which includes update protocols for both success and failure.\n",
    "# Experiment with squeeze on/off and with magnitude of reinforcement for regulator action. \n",
    "# With squeeze on, it might need to be len(regulator_dict), whereas the sqrt of that might be sufficient without squeeze.\n",
    "def update(regulator_dict,action,out,goal,urn_list,skweez=False):\n",
    "    success = 0\n",
    "    if out == goal:\n",
    "        #print(action)\n",
    "        #print(\"success: reinforced the regulator's action\", action, \"from\", regulator_dict[action], \"to\", regulator_dict[action]+len(regulator_dict))\n",
    "        regulator_dict[action] += len(regulator_dict)**(1/2)\n",
    "        success += 1\n",
    "        print(\"success!\")\n",
    "        #print('now we need to recalculate the probabilities according to the reinforced urn')\n",
    "    elif skweez:\n",
    "        print('fail: squeezing.')\n",
    "        squeeze(regulator_dict, urn_list)\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update(regulator,reg_action,out,goal,urn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob_calc(regulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original probabilities\n",
    "#probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets repetitively train a regulator on multiple disturbances from the environment.\n",
    "# After each outcome, we will update, hoping to improve the likelihood of successful plays.\n",
    "# This function incorporates most of what we have done separately above.\n",
    "\n",
    "def train(game_size,goal,epochs,skweez):\n",
    "    game = create_game(game_size)\n",
    "    print(game)\n",
    "    urn = np.random.randint(100, size=len(game.columns))\n",
    "    probs = np.array([(i/sum(urn)) for i in urn])\n",
    "    #print(\"probs:\",probs)\n",
    "    regulator = dict(zip(game.columns,urn))\n",
    "    print(\"regulator:\",regulator)\n",
    "    dist = np.random.dirichlet(alpha=game.index)\n",
    "    successes = 0\n",
    "    i=1\n",
    "    while i <= epochs:\n",
    "        \n",
    "        print(\"Epoch: \",i)\n",
    "        \n",
    "        # Environment chooses play.\n",
    "        play = environment_play(game,dist)\n",
    "        \n",
    "        # Regulator chooses action.\n",
    "        action = regulator_action(game,probs)\n",
    "        \n",
    "        # Compute state of the world that is output (index in game matrix)\n",
    "        out = game.loc[play,action]\n",
    "        #print(\"out:\",out)\n",
    "        \n",
    "        # Update regulator.\n",
    "        successes += update(regulator,action,out,goal,urn,skweez=skweez)\n",
    "        print(\"successes per epoch:\",successes / i)\n",
    "        \n",
    "        # Recalculate regulator probabilities.\n",
    "        probs = prob_calc(regulator)\n",
    "        #print(\"updated probs:\",probs)\n",
    "        \n",
    "        \n",
    "        #Increment i.\n",
    "        i += 1\n",
    "    return regulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "Notice the accuracy improvement when the regulator has access to more \"plays\" (columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "    1    2    3    4    5    6    7    8    9    10   ...  191  192  193  194  \\\n",
      "1     1    5    0    5    5    6    4    5    0    5  ...    9    6    4    2   \n",
      "2     0    6    8    8    5    5    5    1    4    9  ...    8    9    9    1   \n",
      "3     9    3    9    2    6    3    9    7    9    1  ...    6    6    2    8   \n",
      "4     2    7    6    8    1    0    7    7    4    3  ...    0    4    2    8   \n",
      "5     1    0    3    8    5    6    8    3    9    5  ...    7    1    3    2   \n",
      "6     8    3    8    4    4    1    0    3    5    8  ...    6    7    4    9   \n",
      "7     2    5    8    8    7    6    9    6    2    0  ...    7    8    3    1   \n",
      "8     7    0    6    2    8    8    6    0    8    9  ...    6    0    8    7   \n",
      "9     4    3    2    6    2    4    3    9    6    0  ...    3    3    6    5   \n",
      "10    5    1    4    5    2    5    8    2    6    6  ...    0    3    3    0   \n",
      "\n",
      "    195  196  197  198  199  200  \n",
      "1     0    1    0    7    7    4  \n",
      "2     1    7    8    4    9    0  \n",
      "3     3    3    6    9    6    1  \n",
      "4     2    3    0    9    3    2  \n",
      "5     2    3    9    7    3    7  \n",
      "6     9    5    8    7    7    6  \n",
      "7     4    4    9    8    5    7  \n",
      "8     7    4    4    2    0    3  \n",
      "9     5    6    3    6    8    8  \n",
      "10    8    6    0    9    5    2  \n",
      "\n",
      "[10 rows x 200 columns]\n",
      "regulator: {1: 80, 2: 9, 3: 70, 4: 86, 5: 27, 6: 5, 7: 83, 8: 20, 9: 86, 10: 43, 11: 70, 12: 20, 13: 24, 14: 47, 15: 72, 16: 72, 17: 6, 18: 16, 19: 97, 20: 29, 21: 22, 22: 17, 23: 21, 24: 80, 25: 23, 26: 82, 27: 50, 28: 70, 29: 13, 30: 31, 31: 4, 32: 41, 33: 14, 34: 37, 35: 95, 36: 82, 37: 0, 38: 72, 39: 22, 40: 87, 41: 11, 42: 13, 43: 91, 44: 36, 45: 62, 46: 14, 47: 11, 48: 89, 49: 49, 50: 20, 51: 42, 52: 69, 53: 14, 54: 60, 55: 71, 56: 18, 57: 28, 58: 77, 59: 83, 60: 73, 61: 49, 62: 21, 63: 13, 64: 83, 65: 42, 66: 12, 67: 34, 68: 21, 69: 5, 70: 56, 71: 65, 72: 82, 73: 72, 74: 40, 75: 47, 76: 36, 77: 5, 78: 72, 79: 88, 80: 78, 81: 31, 82: 21, 83: 97, 84: 27, 85: 4, 86: 85, 87: 78, 88: 83, 89: 2, 90: 61, 91: 73, 92: 70, 93: 3, 94: 36, 95: 31, 96: 2, 97: 15, 98: 25, 99: 90, 100: 79, 101: 12, 102: 88, 103: 36, 104: 38, 105: 5, 106: 62, 107: 25, 108: 50, 109: 12, 110: 74, 111: 88, 112: 0, 113: 26, 114: 77, 115: 57, 116: 28, 117: 9, 118: 8, 119: 73, 120: 60, 121: 13, 122: 35, 123: 6, 124: 21, 125: 54, 126: 76, 127: 82, 128: 2, 129: 43, 130: 60, 131: 75, 132: 68, 133: 24, 134: 37, 135: 27, 136: 42, 137: 9, 138: 4, 139: 82, 140: 34, 141: 4, 142: 73, 143: 94, 144: 72, 145: 62, 146: 6, 147: 91, 148: 73, 149: 23, 150: 95, 151: 21, 152: 57, 153: 23, 154: 30, 155: 90, 156: 43, 157: 84, 158: 13, 159: 0, 160: 88, 161: 32, 162: 37, 163: 90, 164: 35, 165: 25, 166: 94, 167: 38, 168: 12, 169: 87, 170: 33, 171: 56, 172: 98, 173: 49, 174: 33, 175: 37, 176: 82, 177: 91, 178: 85, 179: 9, 180: 62, 181: 63, 182: 33, 183: 49, 184: 21, 185: 32, 186: 65, 187: 26, 188: 42, 189: 26, 190: 37, 191: 36, 192: 63, 193: 9, 194: 40, 195: 99, 196: 41, 197: 89, 198: 43, 199: 4, 200: 75}\n",
      "Epoch:  1\n",
      "successes per epoch: 0.0\n",
      "Epoch:  2\n",
      "successes per epoch: 0.0\n",
      "Epoch:  3\n",
      "success!\n",
      "successes per epoch: 0.3333333333333333\n",
      "Epoch:  4\n",
      "successes per epoch: 0.25\n",
      "Epoch:  5\n",
      "success!\n",
      "successes per epoch: 0.4\n",
      "Epoch:  6\n",
      "successes per epoch: 0.3333333333333333\n",
      "Epoch:  7\n",
      "successes per epoch: 0.2857142857142857\n",
      "Epoch:  8\n",
      "successes per epoch: 0.25\n",
      "Epoch:  9\n",
      "successes per epoch: 0.2222222222222222\n",
      "Epoch:  10\n",
      "successes per epoch: 0.2\n"
     ]
    }
   ],
   "source": [
    "trained_regulator = train(game_size=(10,200),goal=5,epochs=10,skweez=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 80,\n",
       " 2: 9,\n",
       " 3: 70,\n",
       " 4: 86,\n",
       " 5: 27,\n",
       " 6: 5,\n",
       " 7: 83,\n",
       " 8: 20,\n",
       " 9: 86,\n",
       " 10: 43,\n",
       " 11: 70,\n",
       " 12: 20,\n",
       " 13: 24,\n",
       " 14: 47,\n",
       " 15: 72,\n",
       " 16: 72,\n",
       " 17: 6,\n",
       " 18: 16,\n",
       " 19: 97,\n",
       " 20: 29,\n",
       " 21: 22,\n",
       " 22: 17,\n",
       " 23: 21,\n",
       " 24: 80,\n",
       " 25: 23,\n",
       " 26: 82,\n",
       " 27: 50,\n",
       " 28: 70,\n",
       " 29: 13,\n",
       " 30: 31,\n",
       " 31: 4,\n",
       " 32: 41,\n",
       " 33: 14,\n",
       " 34: 37,\n",
       " 35: 95,\n",
       " 36: 82,\n",
       " 37: 0,\n",
       " 38: 72,\n",
       " 39: 22,\n",
       " 40: 87,\n",
       " 41: 11,\n",
       " 42: 13,\n",
       " 43: 91,\n",
       " 44: 36,\n",
       " 45: 62,\n",
       " 46: 14,\n",
       " 47: 11,\n",
       " 48: 89,\n",
       " 49: 49,\n",
       " 50: 20,\n",
       " 51: 42,\n",
       " 52: 69,\n",
       " 53: 14,\n",
       " 54: 60,\n",
       " 55: 71,\n",
       " 56: 18,\n",
       " 57: 28,\n",
       " 58: 77,\n",
       " 59: 83,\n",
       " 60: 73,\n",
       " 61: 49,\n",
       " 62: 21,\n",
       " 63: 13,\n",
       " 64: 83,\n",
       " 65: 42,\n",
       " 66: 12,\n",
       " 67: 34,\n",
       " 68: 21,\n",
       " 69: 5,\n",
       " 70: 56,\n",
       " 71: 65,\n",
       " 72: 82,\n",
       " 73: 72,\n",
       " 74: 40,\n",
       " 75: 47,\n",
       " 76: 36,\n",
       " 77: 5,\n",
       " 78: 72,\n",
       " 79: 88,\n",
       " 80: 78,\n",
       " 81: 31,\n",
       " 82: 21,\n",
       " 83: 111.14213562373095,\n",
       " 84: 27,\n",
       " 85: 4,\n",
       " 86: 99.14213562373095,\n",
       " 87: 78,\n",
       " 88: 83,\n",
       " 89: 2,\n",
       " 90: 61,\n",
       " 91: 73,\n",
       " 92: 70,\n",
       " 93: 3,\n",
       " 94: 36,\n",
       " 95: 31,\n",
       " 96: 2,\n",
       " 97: 15,\n",
       " 98: 25,\n",
       " 99: 90,\n",
       " 100: 79,\n",
       " 101: 12,\n",
       " 102: 88,\n",
       " 103: 36,\n",
       " 104: 38,\n",
       " 105: 5,\n",
       " 106: 62,\n",
       " 107: 25,\n",
       " 108: 50,\n",
       " 109: 12,\n",
       " 110: 74,\n",
       " 111: 88,\n",
       " 112: 0,\n",
       " 113: 26,\n",
       " 114: 77,\n",
       " 115: 57,\n",
       " 116: 28,\n",
       " 117: 9,\n",
       " 118: 8,\n",
       " 119: 73,\n",
       " 120: 60,\n",
       " 121: 13,\n",
       " 122: 35,\n",
       " 123: 6,\n",
       " 124: 21,\n",
       " 125: 54,\n",
       " 126: 76,\n",
       " 127: 82,\n",
       " 128: 2,\n",
       " 129: 43,\n",
       " 130: 60,\n",
       " 131: 75,\n",
       " 132: 68,\n",
       " 133: 24,\n",
       " 134: 37,\n",
       " 135: 27,\n",
       " 136: 42,\n",
       " 137: 9,\n",
       " 138: 4,\n",
       " 139: 82,\n",
       " 140: 34,\n",
       " 141: 4,\n",
       " 142: 73,\n",
       " 143: 94,\n",
       " 144: 72,\n",
       " 145: 62,\n",
       " 146: 6,\n",
       " 147: 91,\n",
       " 148: 73,\n",
       " 149: 23,\n",
       " 150: 95,\n",
       " 151: 21,\n",
       " 152: 57,\n",
       " 153: 23,\n",
       " 154: 30,\n",
       " 155: 90,\n",
       " 156: 43,\n",
       " 157: 84,\n",
       " 158: 13,\n",
       " 159: 0,\n",
       " 160: 88,\n",
       " 161: 32,\n",
       " 162: 37,\n",
       " 163: 90,\n",
       " 164: 35,\n",
       " 165: 25,\n",
       " 166: 94,\n",
       " 167: 38,\n",
       " 168: 12,\n",
       " 169: 87,\n",
       " 170: 33,\n",
       " 171: 56,\n",
       " 172: 98,\n",
       " 173: 49,\n",
       " 174: 33,\n",
       " 175: 37,\n",
       " 176: 82,\n",
       " 177: 91,\n",
       " 178: 85,\n",
       " 179: 9,\n",
       " 180: 62,\n",
       " 181: 63,\n",
       " 182: 33,\n",
       " 183: 49,\n",
       " 184: 21,\n",
       " 185: 32,\n",
       " 186: 65,\n",
       " 187: 26,\n",
       " 188: 42,\n",
       " 189: 26,\n",
       " 190: 37,\n",
       " 191: 36,\n",
       " 192: 63,\n",
       " 193: 9,\n",
       " 194: 40,\n",
       " 195: 99,\n",
       " 196: 41,\n",
       " 197: 89,\n",
       " 198: 43,\n",
       " 199: 4,\n",
       " 200: 75}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_regulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
